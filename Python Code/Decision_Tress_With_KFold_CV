#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb 19 14:39:59 2018

@author: sasha
"""
#%reset

from Class_tree import DecisionTree
from Class_replace_impute_encode import ReplaceImputeEncode
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from sklearn.model_selection import cross_val_score
from pydotplus import graph_from_dot_data
import graphviz as show_tree
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np


file_path = '/Users/sasha/Library/Mobile Documents/com~apple~CloudDocs/STAT 656/Data/'
df = pd.read_excel(file_path+"CreditHistory_Clean.xlsx")
df.dtypes
cat_map = {'good':0,'bad':1}
# drop target and purpose
X = df.drop('good_bad', axis=1)
X = X.drop('purpose', axis=1)

#save target varaiable to y
y = df['good_bad'].map(cat_map)


# First Integer Designates Data Type
# 0=Interval, 1=Binary, 2=Nominal, 3=Other (No Changes, do not include)
attribute_map = {
    'age':[0,(0,120),[0,0]],
    'amount':[0,(0,20000),[0,0]],
    'duration':[0,(1,1000),[0,0]],
    'checking':[2,(1,2,3,4),[0,0]],
    'coapp':[2,(1,2,3),[0,0]],
    'depends':[1,(1,2),[0,0]],
    'employed':[2,(1,2,3,4,5),[0,0]],
    'existcr':[2,(1,2,3,4),[0,0]],
    'foreign':[1,(1,2),[0,0]],
    'history':[2,(0,1,2,3,4),[0,0]],
   # 'purpose':[2,(0,1,2,3,4,5,6,7,8,9,10),[0,0]],
    'housing':[2,(1,2,3),[0,0]],
    'installp':[2,(1,2,3,4),[0,0]],
    'job':[2,(1,2,3,4),[0,0]],
    'marital':[2,(1,2,3,4),[0,0]],
    'other':[2,(1,2,3),[0,0]],
    'property':[2,(1,2,3,4),[0,0]],
    'resident':[2,(1,2,3,4),[0,0]],
    'savings':[2,(1,2,3,4,5),[0,0]],
    'telephon':[1,(1,2),[0,0]]
    }

#do one-hot encoding
encoding = 'one-hot'# Categorical encoding:  Use 'SAS', 'one-hot' or None
scale    = None     # Interval scaling:  Use 'std', 'robust' or None

rie = ReplaceImputeEncode(data_map=attribute_map, nominal_encoding=encoding, \
                          interval_scale = scale, display=True,drop=False)

# Now request replace-impute-encode for your dataframe
encoded_df = rie.fit_transform(X)
                
X = encoded_df

#set depth vector
depth=[5,6,7,8,10,12,15,20,25]

#split 70, 30 and use X_train and y_train for doing 10 fold cross validation
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state=7)

np.random.seed(7)
#find statistcs for each depth
for i in depth:  
    dtc = DecisionTreeClassifier(criterion='gini', max_depth=i, \
    min_samples_split=5, min_samples_leaf=5)
    dtc = dtc.fit(X_train,y_train)

    score_list = ['accuracy', 'recall', 'precision', 'f1']
    mean_score = []
    std_score = []
    print("\n{:.<13s}{:>6s}{:>13s}".format("Metric", "Mean", "Std. Dev."))
    print("---- depth = "+str(i)+" ----")
   
    for s in score_list:        
        dtc_10 = cross_val_score(dtc, X_train, y_train, scoring=s, cv=10)
        mean = dtc_10.mean()
        std = dtc_10.std()
        mean_score.append(mean)
        std_score.append(std)
        print("{:.<13s}{:>7.4f}{:>10.4f}".format(s, mean, std))

#best model at depth = 10 
#precision and recall values are more at depth=10. Also F1 score is high. It means that the false positive and false negative values
#are less. So we can accurately identifying a customer with good credit.          
dtc = DecisionTreeClassifier(criterion='gini', max_depth=12, \
min_samples_split=5, min_samples_leaf=5)
dtc = dtc.fit(X_train,y_train)
features = list(X)
classes = ['good', 'bad']

DecisionTree.display_importance(dtc, features)
dot_data = export_graphviz(dtc, filled=True, rounded=True, \
class_names=classes, feature_names = features, out_file=None)


#write tree to png file 
graph_png = graph_from_dot_data(dot_data)
graph_path = '/Users/sasha/Desktop/Python/'
graph_png.write_png(graph_path+'TreeDepth_depth10.png')

#Display Tree
graph_pdf = show_tree.Source(dot_data)
graph_pdf.view('CreditHistoryTree') #Displays tree

#Get statistics
DecisionTree.display_binary_metrics(dtc, X_test, y_test)


        
